{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class STModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, emd_dim, vocab_size, classes_num, hidden_num, layer_num, embeddings = None, padding_idx = 0,dropout=0.5):\n",
    "        super(STModel, self).__init__()\n",
    "        self.encoder = nn.Embedding(vocab_size, emd_dim, padding_idx= padding_idx, _weight = embeddings)\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        self.lstm = nn.LSTM(emd_dim, hidden_num, layer_num, dropout= dropout,\n",
    "                            bidirectional= False)\n",
    "        self.fc = nn.Linear(emd_dim, classes_num)\n",
    "        self.init_weights()\n",
    "        self.sigm = nn.Sigmoid()\n",
    "        self.hidden_num = hidden_num\n",
    "        \n",
    "    def forward(self, x, hidden):\n",
    "        batch_size = x.size(0)\n",
    "        emb = self.drop(self.encoder(x))\n",
    "        lstm_out, hidden = self.lstm(emb, hidden)\n",
    "        lstm_out = self.drop(lstm_out) #[seq_len, batch, num_directions * hidden_size];\n",
    "        fc_input = lstm_out[-1]\n",
    "        fc_out = self.fc(lstm_out)\n",
    "        sig_out = self.sigm(fc_out)\n",
    "        pred = sig_out.view(batch_size, -1)\n",
    "        return sig_out\n",
    "    \n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        #self.encoder.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc.bias.data.zero_()\n",
    "        self.fc.weight.data.uniform_(-initrange, initrange)\n",
    "        \n",
    "    def init_hidden(self, batch_size):\n",
    "        weight = next(self.parameters())\n",
    "        return (weight.new_zeros(self.layer_num, batch_size, self.hidden_num),\n",
    "                    weight.new_zeros(self.layer_num, batch_size, self.hidden_num))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "emd_dim = 300\n",
    "vocab_size = 100\n",
    "classes_num = 3\n",
    "hidden_num = 100\n",
    "layer_num = 2\n",
    "dropout = 0.5\n",
    "net = STModel(emd_dim, vocab_size, classes_num, hidden_num, layer_num, dropout=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STModel(\n",
      "  (encoder): Embedding(100, 300)\n",
      "  (drop): Dropout(p=0.5, inplace=False)\n",
      "  (lstm): LSTM(300, 100, num_layers=2, dropout=0.5)\n",
      "  (fc): Linear(in_features=300, out_features=3, bias=True)\n",
      "  (sigm): Sigmoid()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "SAVE_PATH = 'STMmodel.pt'\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "clip = 5\n",
    "epochs = 5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(epochs):\n",
    "    running_loss = 0.0\n",
    "    batch_count = 0\n",
    "    for i, train_data in enumerate(trainloader):\n",
    "        train_input, train_laebl = train_data\n",
    "        # forward\n",
    "        hidden = model.init_hidden(batch_size) # 有问题\n",
    "        model.zero_grad()\n",
    "        outputs = model(train_input, hidden)\n",
    "        loss = criterion(outputs, labels)\n",
    "        # backward\n",
    "        loss.backward()\n",
    "        # optimize\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' %(epoch + 1, i + 1, running_loss / 2000))\n",
    "            running_loss = 0.0\n",
    "            torch.save(net.state_dict(), PATH)\n",
    "            with open(SAVE_PATH, 'wb') as f:\n",
    "                torch.save(model, f)\n",
    "print('finished training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = pd.read_csv('data/train_weibo_clean.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>微博id</th>\n",
       "      <th>微博发布时间</th>\n",
       "      <th>发布人账号</th>\n",
       "      <th>微博中文内容</th>\n",
       "      <th>微博图片</th>\n",
       "      <th>微博视频</th>\n",
       "      <th>情感倾向</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4456072029125500</td>\n",
       "      <td>01月01日 23:50</td>\n",
       "      <td>存曦1988</td>\n",
       "      <td>写在年末冬初孩子流感的第五天，我们仍然没有忘记热情拥抱这2020年的第一天。带着一丝迷信，早...</td>\n",
       "      <td>['https://ww2.sinaimg.cn/orj360/005VnA1zly1gah...</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4456074167480980</td>\n",
       "      <td>01月01日 23:58</td>\n",
       "      <td>LunaKrys</td>\n",
       "      <td>开年大模型…累到以为自己发烧了腰疼膝盖疼腿疼胳膊疼脖子疼#Luna的Krystallife#?</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4456054253264520</td>\n",
       "      <td>01月01日 22:39</td>\n",
       "      <td>小王爷学辩论o_O</td>\n",
       "      <td>邱晨这就是我爹，爹，发烧快好，毕竟美好的假期拿来养病不太好，假期还是要好好享受快乐，爹，新...</td>\n",
       "      <td>['https://ww2.sinaimg.cn/thumb150/006ymYXKgy1g...</td>\n",
       "      <td>[]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4456061509126470</td>\n",
       "      <td>01月01日 23:08</td>\n",
       "      <td>芩鎟</td>\n",
       "      <td>新年的第一天感冒又发烧的也太衰了但是我要想着明天一定会好的?</td>\n",
       "      <td>['https://ww2.sinaimg.cn/orj360/005FL9LZgy1gah...</td>\n",
       "      <td>[]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4455979322528190</td>\n",
       "      <td>01月01日 17:42</td>\n",
       "      <td>changlwj</td>\n",
       "      <td>问：我们意念里有坏的想法了，天神就会给记下来，那如果有好的想法也会被记下来吗？答：那当然了。...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>99908</td>\n",
       "      <td>99995</td>\n",
       "      <td>4473033438259880</td>\n",
       "      <td>02月17日 19:08</td>\n",
       "      <td>中国教育新闻网</td>\n",
       "      <td>#抗击新型肺炎第一线#【@中国计量大学研制新冠病毒蛋白标准样品】记者从中国计量大学获悉，新型...</td>\n",
       "      <td>['https://ww1.sinaimg.cn/orj360/682cebefly1gbz...</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>99909</td>\n",
       "      <td>99996</td>\n",
       "      <td>4472969222714290</td>\n",
       "      <td>02月17日 14:53</td>\n",
       "      <td>fuzhuoting</td>\n",
       "      <td>1、类RaTG13病毒（一种从云南蝙蝠身上分离出来的冠状病毒）可能是2019-nCoV的源头...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>99910</td>\n",
       "      <td>99997</td>\n",
       "      <td>4473035904435920</td>\n",
       "      <td>02月17日 19:18</td>\n",
       "      <td>蝌蚪五线谱</td>\n",
       "      <td>#微博辟谣#没有证据表明，吃大蒜、漱口水、涂抹芝麻油、生理盐水洗鼻子等手段可以防止感染新型冠...</td>\n",
       "      <td>['https://ww4.sinaimg.cn/orj360/6d2cc4e6ly1gbz...</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>99911</td>\n",
       "      <td>99998</td>\n",
       "      <td>4472950743017610</td>\n",
       "      <td>02月17日 13:40</td>\n",
       "      <td>医库</td>\n",
       "      <td>【新冠疫情最受关注的十一篇英文核心期刊论文全解析】本文整理了关于新型冠状病毒最受关注的十一篇...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>99912</td>\n",
       "      <td>99999</td>\n",
       "      <td>4472870103356260</td>\n",
       "      <td>02月17日 08:19</td>\n",
       "      <td>有度为王</td>\n",
       "      <td>从蝙蝠携带的冠状病毒变异成2019-nCoV冠状病毒,怎样才能发生变异呢？有两种可能1.自然...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>99913 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0              微博id        微博发布时间       发布人账号  \\\n",
       "0               0  4456072029125500  01月01日 23:50      存曦1988   \n",
       "1               1  4456074167480980  01月01日 23:58    LunaKrys   \n",
       "2               2  4456054253264520  01月01日 22:39   小王爷学辩论o_O   \n",
       "3               3  4456061509126470  01月01日 23:08          芩鎟   \n",
       "4               4  4455979322528190  01月01日 17:42    changlwj   \n",
       "...           ...               ...           ...         ...   \n",
       "99908       99995  4473033438259880  02月17日 19:08     中国教育新闻网   \n",
       "99909       99996  4472969222714290  02月17日 14:53  fuzhuoting   \n",
       "99910       99997  4473035904435920  02月17日 19:18       蝌蚪五线谱   \n",
       "99911       99998  4472950743017610  02月17日 13:40          医库   \n",
       "99912       99999  4472870103356260  02月17日 08:19        有度为王   \n",
       "\n",
       "                                                  微博中文内容  \\\n",
       "0      写在年末冬初孩子流感的第五天，我们仍然没有忘记热情拥抱这2020年的第一天。带着一丝迷信，早...   \n",
       "1        开年大模型…累到以为自己发烧了腰疼膝盖疼腿疼胳膊疼脖子疼#Luna的Krystallife#?   \n",
       "2      邱晨这就是我爹，爹，发烧快好，毕竟美好的假期拿来养病不太好，假期还是要好好享受快乐，爹，新...   \n",
       "3                         新年的第一天感冒又发烧的也太衰了但是我要想着明天一定会好的?   \n",
       "4      问：我们意念里有坏的想法了，天神就会给记下来，那如果有好的想法也会被记下来吗？答：那当然了。...   \n",
       "...                                                  ...   \n",
       "99908  #抗击新型肺炎第一线#【@中国计量大学研制新冠病毒蛋白标准样品】记者从中国计量大学获悉，新型...   \n",
       "99909  1、类RaTG13病毒（一种从云南蝙蝠身上分离出来的冠状病毒）可能是2019-nCoV的源头...   \n",
       "99910  #微博辟谣#没有证据表明，吃大蒜、漱口水、涂抹芝麻油、生理盐水洗鼻子等手段可以防止感染新型冠...   \n",
       "99911  【新冠疫情最受关注的十一篇英文核心期刊论文全解析】本文整理了关于新型冠状病毒最受关注的十一篇...   \n",
       "99912  从蝙蝠携带的冠状病毒变异成2019-nCoV冠状病毒,怎样才能发生变异呢？有两种可能1.自然...   \n",
       "\n",
       "                                                    微博图片 微博视频  情感倾向  \n",
       "0      ['https://ww2.sinaimg.cn/orj360/005VnA1zly1gah...   []     0  \n",
       "1                                                     []   []    -1  \n",
       "2      ['https://ww2.sinaimg.cn/thumb150/006ymYXKgy1g...   []     1  \n",
       "3      ['https://ww2.sinaimg.cn/orj360/005FL9LZgy1gah...   []     1  \n",
       "4                                                     []   []     1  \n",
       "...                                                  ...  ...   ...  \n",
       "99908  ['https://ww1.sinaimg.cn/orj360/682cebefly1gbz...   []     0  \n",
       "99909                                                 []   []     0  \n",
       "99910  ['https://ww4.sinaimg.cn/orj360/6d2cc4e6ly1gbz...   []     0  \n",
       "99911                                                 []   []     1  \n",
       "99912                                                 []   []     0  \n",
       "\n",
       "[99913 rows x 8 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/13/33/ffb67897a6985a7b7d8e5e7878c3628678f553634bd3836404fef06ef19b/transformers-2.5.1-py3-none-any.whl (499kB)\n",
      "\u001b[K     |████████████████████████████████| 501kB 42kB/s eta 0:00:011\n",
      "\u001b[?25hCollecting tokenizers==0.5.2 (from transformers)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d6/e3/5e49e9a83fb605aaa34a1c1173e607302fecae529428c28696fb18f1c2c9/tokenizers-0.5.2-cp37-cp37m-manylinux1_x86_64.whl (5.6MB)\n",
      "\u001b[K     |████████████████████████████████| 5.6MB 11kB/s eta 0:00:0163\n",
      "\u001b[?25hCollecting sentencepiece (from transformers)\n",
      "\u001b[33m  WARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'SSLError(SSLError(\"bad handshake: SysCallError(104, 'ECONNRESET')\"))': /simple/sentencepiece/\u001b[0m\n",
      "\u001b[33m  WARNING: Retrying (Retry(total=3, connect=None, read=None, redirect=None, status=None)) after connection broken by 'SSLError(SSLError(\"bad handshake: SysCallError(104, 'ECONNRESET')\"))': /simple/sentencepiece/\u001b[0m\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/11/e0/1264990c559fb945cfb6664742001608e1ed8359eeec6722830ae085062b/sentencepiece-0.1.85-cp37-cp37m-manylinux1_x86_64.whl (1.0MB)\n",
      "\u001b[K     |████████████████████████████████| 1.0MB 11kB/s eta 0:00:016\n",
      "\u001b[?25hRequirement already satisfied: numpy in /home/leslie/anaconda3/lib/python3.7/site-packages (from transformers) (1.17.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/leslie/anaconda3/lib/python3.7/site-packages (from transformers) (4.36.1)\n",
      "Requirement already satisfied: requests in /home/leslie/anaconda3/lib/python3.7/site-packages (from transformers) (2.22.0)\n",
      "Collecting sacremoses (from transformers)\n",
      "\u001b[33m  WARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='pypi.org', port=443): Read timed out. (read timeout=15)\")': /simple/sacremoses/\u001b[0m\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a6/b4/7a41d630547a4afd58143597d5a49e07bfd4c42914d8335b2a5657efc14b/sacremoses-0.0.38.tar.gz (860kB)\n",
      "\u001b[K     |████████████████████████████████| 870kB 5.5kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: filelock in /home/leslie/anaconda3/lib/python3.7/site-packages (from transformers) (3.0.12)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/49/6b/b8dbb406ff9c1df97ca591050ba9b7252b1b4d491555d182d463ea72cc11/regex-2020.2.20-cp37-cp37m-manylinux2010_x86_64.whl (689kB)\n",
      "\u001b[K     |████████████████████████████████| 696kB 10kB/s eta 0:00:012\n",
      "\u001b[?25hRequirement already satisfied: boto3 in /home/leslie/anaconda3/lib/python3.7/site-packages (from transformers) (1.11.12)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /home/leslie/anaconda3/lib/python3.7/site-packages (from requests->transformers) (1.24.2)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /home/leslie/anaconda3/lib/python3.7/site-packages (from requests->transformers) (2.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/leslie/anaconda3/lib/python3.7/site-packages (from requests->transformers) (2019.9.11)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /home/leslie/anaconda3/lib/python3.7/site-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: six in /home/leslie/anaconda3/lib/python3.7/site-packages (from sacremoses->transformers) (1.12.0)\n",
      "Requirement already satisfied: click in /home/leslie/anaconda3/lib/python3.7/site-packages (from sacremoses->transformers) (7.0)\n",
      "Requirement already satisfied: joblib in /home/leslie/anaconda3/lib/python3.7/site-packages (from sacremoses->transformers) (0.13.2)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /home/leslie/anaconda3/lib/python3.7/site-packages (from boto3->transformers) (0.9.4)\n",
      "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /home/leslie/anaconda3/lib/python3.7/site-packages (from boto3->transformers) (0.3.3)\n",
      "Requirement already satisfied: botocore<1.15.0,>=1.14.12 in /home/leslie/anaconda3/lib/python3.7/site-packages (from boto3->transformers) (1.14.12)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /home/leslie/anaconda3/lib/python3.7/site-packages (from botocore<1.15.0,>=1.14.12->boto3->transformers) (2.8.0)\n",
      "Requirement already satisfied: docutils<0.16,>=0.10 in /home/leslie/anaconda3/lib/python3.7/site-packages (from botocore<1.15.0,>=1.14.12->boto3->transformers) (0.15.2)\n",
      "Building wheels for collected packages: sacremoses\n",
      "  Building wheel for sacremoses (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for sacremoses: filename=sacremoses-0.0.38-cp37-none-any.whl size=884628 sha256=005d48058e970b5f56f719e958c39559fd1666881e2fba9a9b5c255d838e66e2\n",
      "  Stored in directory: /home/leslie/.cache/pip/wheels/6d/ec/1a/21b8912e35e02741306f35f66c785f3afe94de754a0eaf1422\n",
      "Successfully built sacremoses\n",
      "Installing collected packages: tokenizers, sentencepiece, regex, sacremoses, transformers\n",
      "Successfully installed regex-2020.2.20 sacremoses-0.0.38 sentencepiece-0.1.85 tokenizers-0.5.2 transformers-2.5.1\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
