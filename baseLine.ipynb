{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"baseLine.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"mount_file_id":"1gXmK2q63N8pMLEGmG60juPtN1B9JWrn-","authorship_tag":"ABX9TyNMllersnnfwrjOQLCSr0Ad"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"ecXO1q3LhUX8","colab_type":"code","colab":{}},"source":["import pandas as pd\n","import numpy as np\n","import pickle"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"J85c2M-7hhhG","colab_type":"code","colab":{}},"source":["import csv\n","def read_data(dataPath, is_train = True):\n","    with open(dataPath, mode='r', encoding='utf-8') as f:\n","        reader = csv.DictReader(f)\n","        data = list(reader)\n","        data = np.asarray(data)\n","        if is_train:\n","          content_data = {'content':[], 'label':[]}\n","        else:\n","          content_data = {'content':[], 'id':[]}\n","        for line in data:\n","            content_data['content'].append(list(line['微博中文内容']))\n","            if is_train:\n","              content_data['label'].append(line['情感倾向'])\n","            else:\n","              content_data['id'].append(line['微博id'])\n","        \n","        return content_data\n","    "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"G3bAXphohbyc","colab_type":"code","colab":{}},"source":["content_train_data = read_data('drive/My Drive/NLP/sentiment_compete/data/train_weibo_clean.csv')\n","content_test_data = read_data('drive/My Drive/NLP/sentiment_compete/data/test_weibo_clean.csv', False)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"oYPQo91-hfLZ","colab_type":"code","colab":{}},"source":["import pickle\n","import numpy as np\n","\n","UNKNOWN = '<UNK>'\n","PADDING = '<PAD>'\n","\n","def read_pretrain_embedding(emb_path):\n","    pre_train_emd = {}\n","    with open(emb_path, mode='r', encoding='utf-8') as f:\n","        word_nums, dim = f.readline().split()\n","        word_nums = int(word_nums)\n","        dim = int(dim)\n","        for line in f:\n","            tokens = line.strip().split(' ')\n","            if len(tokens) == dim + 1:\n","                pre_train_emd[tokens[0]] = list(map(lambda x: float(x), tokens[1:]))\n","    return pre_train_emd"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"R74bvC_ekj0C","colab_type":"code","outputId":"ef9aee7c-a191-40e6-ec52-462d81b204c2","executionInfo":{"status":"error","timestamp":1583661353924,"user_tz":-480,"elapsed":1691,"user":{"displayName":"Rong Leslie","photoUrl":"","userId":"04972105603495756887"}},"colab":{"base_uri":"https://localhost:8080/","height":284}},"source":["pre_train_emd = read_pretrain_embedding('data/sgns.weibo.char')"],"execution_count":0,"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-23-6210c73712b0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpre_train_emd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_pretrain_embedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data/sgns.weibo.char'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-22-8d00ff1df286>\u001b[0m in \u001b[0;36mread_pretrain_embedding\u001b[0;34m(emb_path)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mread_pretrain_embedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0memb_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mpre_train_emd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0memb_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'r'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0mword_nums\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mword_nums\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_nums\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/sgns.weibo.char'"]}]},{"cell_type":"code","metadata":{"id":"nSlTQMGjhp_f","colab_type":"code","colab":{}},"source":["def build_word_emb(content_train_data, pre_train_emd, emd_dim, content_test_data = None):\n","    \"\"\"\n","        return word_emb type-list [n_vocab * dim]\n","               word2Index type-dict {word: index}\n","    \"\"\"\n","    word_emb = [np.random.uniform(-0.1, 0.1, emd_dim) for _ in range(2)]\n","    word2Index = {PADDING: 0, UNKNOWN: 1}\n","    for content in content_train_data['content']:\n","        for i in range(len(content)):\n","            if content[i] not in word2Index:\n","                if content[i] in pre_train_emd: # in vocab\n","                    word2Index[content[i]] = len(word2Index)\n","                    word_emb.append(pre_train_emd[content[i]])\n","                else: # not in vocab\n","                    content[i] = UNKNOWN\n","            content[i] = word2Index[content[i]]\n","            \n","    if content_test_data is not None:\n","        for content in content_test_data['content']:\n","            for i in range(len(content)):\n","                if content[i] not in word2Index:\n","                    if content[i] in pre_train_emd: # in vocab\n","                        word2Index[content[i]] = len(word2Index)\n","                        word_emb.append(pre_train_emd[content[i]])\n","                    else: # not in vocab\n","                        content[i] = UNKNOWN\n","                content[i] = word2Index[content[i]]\n","    word_emb = np.asarray(word_emb, dtype='float32')\n","    return word_emb, word2Index"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"xF7UCtLzirT1","colab_type":"code","colab":{}},"source":["def save_word_emb(word_emb, word2Index):\n","    with open('data/word_emb', 'wb') as f:\n","        pickle.dump((word_emb, word2Index), f)\n","    print('saved.')\n","\n","def load_word_emb(path):\n","    with open(path, 'rb') as f:\n","        word_emb, word2Index = pickle.load(f)\n","        return word_emb, word2Index\n","\n","def save_doc_index(path, content_train_data, content_test_data):\n","    with open(path, 'wb') as f:\n","        pickle.dump((content_train_data, content_test_data), f)\n","    print('saved.')\n","\n","def load_doc_index(path):\n","    with open(path, 'rb') as f:\n","        content_train_data, content_test_data = pickle.load(f)\n","        return word_emb, word2Index"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"CgrgiAXskT0B","colab_type":"code","outputId":"8e7c58c8-2e6d-4f22-b1db-c3b9594d39eb","executionInfo":{"status":"error","timestamp":1583737611292,"user_tz":-480,"elapsed":815,"user":{"displayName":"Rong Leslie","photoUrl":"","userId":"04972105603495756887"}},"colab":{"base_uri":"https://localhost:8080/","height":185}},"source":["word_emb, word2Index = build_word_emb(content_train_data, pre_train_emd, 300, content_test_data)"],"execution_count":0,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-107-10bb58f9871b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mword_emb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword2Index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_word_emb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontent_train_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpre_train_emd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontent_test_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'pre_train_emd' is not defined"]}]},{"cell_type":"code","metadata":{"id":"OinBLZtEixDv","colab_type":"code","colab":{}},"source":["word_emb, word2Index = load_word_emb('drive/My Drive/NLP/sentiment_compete/data/word_emb')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"za95MVaui29E","colab_type":"code","colab":{}},"source":["def pad_content(content_data, seq_len, word2Index):\n","    pad_seq = np.zeros((len(content_data), seq_len), dtype=int)\n","    for i, row in enumerate(content_data):\n","        doc_len = seq_len\n","        if doc_len > len(row):\n","            doc_len = len(row)\n","        \n","        for index in range(len(row)):\n","          if row[index] in word2Index:\n","             row[index] = word2Index[row[index]]\n","          else:\n","             row[index] = word2Index[UNKNOWN]\n","        pad_seq[i, :doc_len] = row[:doc_len]\n","    return pad_seq"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"YgtJ7tFMi-HB","colab_type":"code","colab":{}},"source":["seq_len = 140\n","\n","train_content = pad_content(content_train_data['content'], seq_len, word2Index)\n","test_content = pad_content(content_test_data['content'], seq_len, word2Index)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"KZ9z01XMEVWu","colab_type":"code","colab":{}},"source":["test_data = list(zip(content_test_data['id'] ,test_content))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"INZWP8TMF0za","colab_type":"code","outputId":"3b228f45-2ba6-4bde-cb48-9ab6af70cf7f","executionInfo":{"status":"ok","timestamp":1583737616924,"user_tz":-480,"elapsed":1347,"user":{"displayName":"Rong Leslie","photoUrl":"","userId":"04972105603495756887"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["train_content.shape"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(99560, 140)"]},"metadata":{"tags":[]},"execution_count":112}]},{"cell_type":"code","metadata":{"id":"mSgeQ4uojDfs","colab_type":"code","colab":{}},"source":["content_train_data['content'] = train_content"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"OAuzqhr7mjOY","colab_type":"code","colab":{}},"source":["def split_data(content_train_data):\n","    np.random.seed(123)\n","    #np.random.shuffle(content_train_data['content'])\n","    #np.random.shuffle(content_train_data['label'])\n","    length = len(content_train_data['content'])\n","    train_x = content_train_data['content'][:int(0.9 * length)]\n","    train_y = content_train_data['label'][:int(0.9 * length)]\n","    valid_x = content_train_data['content'][int(0.9 * length):]\n","    valid_y = content_train_data['label'][int(0.9 * length):]\n","    train_y = np.asarray(train_y, dtype='int')\n","    valid_y = np.asarray(valid_y, dtype='int')\n","    return train_x, train_y, valid_x, valid_y"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"a231UaKbmzuS","colab_type":"code","colab":{}},"source":["train_x, train_y, valid_x, valid_y = split_data(content_train_data)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"JRDWcEJVm1jt","colab_type":"code","colab":{}},"source":["train_y = train_y + 1 # 0 neg 1 mid 2 positive\n","valid_y = valid_y + 1"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"PXUX-o1O4LNU","colab_type":"code","outputId":"d30de485-22b7-4deb-e34f-dfb67fc09148","executionInfo":{"status":"ok","timestamp":1583737620524,"user_tz":-480,"elapsed":851,"user":{"displayName":"Rong Leslie","photoUrl":"","userId":"04972105603495756887"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["valid_x.shape"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(9956, 140)"]},"metadata":{"tags":[]},"execution_count":117}]},{"cell_type":"code","metadata":{"id":"OmOeTSoLZdT7","colab_type":"code","outputId":"2028c8e1-febe-4b50-d2c3-e0e403560d09","executionInfo":{"status":"ok","timestamp":1583723845612,"user_tz":-480,"elapsed":668,"user":{"displayName":"Rong Leslie","photoUrl":"","userId":"04972105603495756887"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["train_y.shape"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(89604,)"]},"metadata":{"tags":[]},"execution_count":90}]},{"cell_type":"code","metadata":{"id":"sOn182Ok6B3w","colab_type":"code","outputId":"bff2939e-7111-4fb3-ec63-4ec284364df5","executionInfo":{"status":"ok","timestamp":1583737624284,"user_tz":-480,"elapsed":1108,"user":{"displayName":"Rong Leslie","photoUrl":"","userId":"04972105603495756887"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["len(train_y[train_y == 2])"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["22305"]},"metadata":{"tags":[]},"execution_count":118}]},{"cell_type":"code","metadata":{"id":"62woQPEkm4Gn","colab_type":"code","colab":{}},"source":["batch_size = 32\n","\n","import torch\n","from torch.utils.data import TensorDataset, DataLoader\n","\n","train_data = TensorDataset(torch.from_numpy(train_x), torch.from_numpy(train_y))\n","valid_data = TensorDataset(torch.from_numpy(valid_x), torch.from_numpy(valid_y))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZGMkA2mPm7-w","colab_type":"code","colab":{}},"source":["train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n","valid_loader = DataLoader(valid_data, shuffle=True, batch_size=batch_size)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"gYoSFTZpm-xv","colab_type":"code","outputId":"0e264165-2c3e-489a-eefe-8794ffc590ed","executionInfo":{"status":"ok","timestamp":1583737629314,"user_tz":-480,"elapsed":1038,"user":{"displayName":"Rong Leslie","photoUrl":"","userId":"04972105603495756887"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["# First checking if GPU is available\n","train_on_gpu=torch.cuda.is_available()\n"," \n","if(train_on_gpu):\n","    print('Training on GPU.')\n","else:\n","    print('No GPU available, training on CPU.')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Training on GPU.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"fN82ORi_pmPl","colab_type":"code","outputId":"5c8bdf8d-1118-4ce3-f8f0-de066cc9cc4a","executionInfo":{"status":"ok","timestamp":1583737631801,"user_tz":-480,"elapsed":1058,"user":{"displayName":"Rong Leslie","photoUrl":"","userId":"04972105603495756887"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","\n","# Assuming that we are on a CUDA machine, this should print a CUDA device:\n","\n","print(device)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["cuda:0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"eMnWv7dtnByx","colab_type":"code","colab":{}},"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","class STModel(nn.Module):\n","    \n","    def __init__(self, emd_dim, vocab_size, classes_num, hidden_num, layer_num, embeddings = None, padding_idx = 0,dropout=0.5):\n","        super(STModel, self).__init__()\n","        self.hidden_num = hidden_num\n","        self.layer_num = layer_num\n","        self.dropout = dropout\n","        self.vocab_size = vocab_size\n","        self.emd_dim = emd_dim\n","        self.encoder = nn.Embedding(vocab_size, emd_dim, padding_idx= padding_idx)\n","        if embeddings is not None:\n","            self.encoder.weight.data.copy_(embeddings)\n","        #self.encoder = nn.Embedding(vocab_size, emd_dim, padding_idx= padding_idx, _weight = embeddings)\n","        self.drop = nn.Dropout(dropout)\n","        self.lstm = nn.LSTM(emd_dim, hidden_num, layer_num, dropout= dropout,\n","                            bidirectional= False,batch_first = True)\n","        self.fc = nn.Linear(hidden_num, classes_num)\n","        self.init_weights()\n","        self.sigm = nn.Sigmoid()\n","        \n","        \n","    def forward(self, x, hidden, print_flag = False):\n","        batch_size = x.size(0)\n","        if(print_flag):\n","          print(batch_size)\n","        emb = self.drop(self.encoder(x))\n","        if(print_flag):\n","          print('emb', emb.size())\n","        if(print_flag):\n","          print('hidden', hidden)\n","        lstm_out, hidden = self.lstm(emb, hidden)\n","        if(print_flag):\n","          print('lstm_out', lstm_out.size())\n","        lstm_out = self.drop(lstm_out) #lstm_out torch.Size([16, 140, 100]) batch seq hidden \n","        fc_input = lstm_out[:,-1,:]\n","        if(print_flag):\n","          print('fc_input', fc_input.size())\n","        fc_out = self.fc(fc_input)\n","        #if(print_flag):\n","       #   print('fc_out', fc_out.size())\n","       # sig_out = self.sigm(fc_out)\n","       # if(print_flag):\n","       #   print('sig_out', sig_out.size())\n","       # pred = sig_out.view(batch_size, -1)\n","       # if(print_flag):\n","        #  print('pred', pred.size())\n","       # return pred\n","        return fc_out\n","    \n","    def init_weights(self):\n","        initrange = 0.1\n","        #self.encoder.weight.data.uniform_(-initrange, initrange)\n","        self.fc.bias.data.zero_()\n","        self.fc.weight.data.uniform_(-initrange, initrange)\n","        \n","    def init_hidden(self, batch_size):\n","        weight = next(self.parameters())\n","        if(train_on_gpu):\n","            return (weight.new_zeros(self.layer_num, batch_size, self.hidden_num).cuda(),\n","                    weight.new_zeros(self.layer_num, batch_size, self.hidden_num).cuda())\n","        else:\n","            return (weight.new_zeros(self.layer_num, batch_size, self.hidden_num),\n","                    weight.new_zeros(self.layer_num, batch_size, self.hidden_num))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"RCcfg_LZnEX_","colab_type":"code","colab":{}},"source":["emd_dim = 300\n","vocab_size = word_emb.shape[0]\n","classes_num = 3\n","hidden_num = 100\n","layer_num = 2\n","dropout = 0.3\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","#embeddings = torch.tensor(pickle.load(pkl), dtype=torch.float).to(device)\n","word_emb = torch.tensor(word_emb, dtype=torch.float,requires_grad=False).to(device)\n","model = STModel(emd_dim, vocab_size, classes_num, hidden_num, layer_num, embeddings = word_emb, dropout = 0.3)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WrCWabcAstQr","colab_type":"text"},"source":["# 新段落"]},{"cell_type":"code","metadata":{"id":"nSryymzDqhoj","colab_type":"code","outputId":"79808090-b073-488d-c830-fd003c1a573e","executionInfo":{"status":"ok","timestamp":1583737642333,"user_tz":-480,"elapsed":782,"user":{"displayName":"Rong Leslie","photoUrl":"","userId":"04972105603495756887"}},"colab":{"base_uri":"https://localhost:8080/","height":136}},"source":["model.to(device)"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["STModel(\n","  (encoder): Embedding(5224, 300, padding_idx=0)\n","  (drop): Dropout(p=0.3, inplace=False)\n","  (lstm): LSTM(300, 100, num_layers=2, batch_first=True, dropout=0.3)\n","  (fc): Linear(in_features=100, out_features=3, bias=True)\n","  (sigm): Sigmoid()\n",")"]},"metadata":{"tags":[]},"execution_count":125}]},{"cell_type":"code","metadata":{"id":"AJ1U1wQQnGHa","colab_type":"code","outputId":"e8ae0beb-b68f-486f-c30f-8eac832c1ba0","executionInfo":{"status":"ok","timestamp":1583737652464,"user_tz":-480,"elapsed":855,"user":{"displayName":"Rong Leslie","photoUrl":"","userId":"04972105603495756887"}},"colab":{"base_uri":"https://localhost:8080/","height":136}},"source":["import torch.optim as optim\n","\n","SAVE_PATH = 'STMmodel.pt'\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(model.parameters(), lr=0.0001)\n","batch_size = 32\n","clip = 5\n","epochs = 100\n","model.train()"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["STModel(\n","  (encoder): Embedding(5224, 300, padding_idx=0)\n","  (drop): Dropout(p=0.3, inplace=False)\n","  (lstm): LSTM(300, 100, num_layers=2, batch_first=True, dropout=0.3)\n","  (fc): Linear(in_features=100, out_features=3, bias=True)\n","  (sigm): Sigmoid()\n",")"]},"metadata":{"tags":[]},"execution_count":126}]},{"cell_type":"code","metadata":{"id":"V9hxJetsnHhM","colab_type":"code","outputId":"2b0d8ead-85af-41fb-bc90-aca251e761e1","executionInfo":{"status":"ok","timestamp":1583676670792,"user_tz":-480,"elapsed":547853,"user":{"displayName":"Rong Leslie","photoUrl":"","userId":"04972105603495756887"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["for epoch in range(epochs):\n","    running_loss = 0.0\n","    batch_count = 0\n","    for i, train_data in enumerate(train_loader):\n","        #print('batch_index:', i)\n","        train_input, train_label = train_data\n","        if(train_on_gpu):\n","            train_input, train_label = train_input.to(device), train_label.to(device)\n","        # forward\n","        hidden = model.init_hidden(train_label.size(0)) # 有问题\n","        model.zero_grad()\n","        outputs = model(train_input, hidden)\n","        loss = criterion(outputs, train_label)\n","        # backward\n","        loss.backward()\n","        # optimize\n","        optimizer.step()\n","        running_loss += loss.item()\n","        if (i+1) % 1000 == 0:    # print every 2000 mini-batches\n","            #val_h = model.init_hidden(batch_size)\n","            evaluate_model(valid_loader, model)\n","            model.train()\n","            print('[%d, %5d] loss: %.3f' %(epoch + 1, i + 1, running_loss / 1000))\n","            running_loss = 0.0\n","            #torch.save(net.state_dict(), PATH)\n","            #with open(SAVE_PATH, 'wb') as f:\n","            #    torch.save(model, f)\n","print('finished training')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Accuracy of the network: 53 %\n","[1,  1000] loss: 0.959\n","Accuracy of the network: 53 %\n","[1,  2000] loss: 0.947\n","Accuracy of the network: 53 %\n","[2,  1000] loss: 0.946\n","Accuracy of the network: 53 %\n","[2,  2000] loss: 0.956\n","Accuracy of the network: 53 %\n","[3,  1000] loss: 0.945\n","Accuracy of the network: 55 %\n","[3,  2000] loss: 0.928\n","Accuracy of the network: 54 %\n","[4,  1000] loss: 0.896\n","Accuracy of the network: 53 %\n","[4,  2000] loss: 0.884\n","Accuracy of the network: 55 %\n","[5,  1000] loss: 0.894\n","Accuracy of the network: 57 %\n","[5,  2000] loss: 0.896\n","Accuracy of the network: 61 %\n","[6,  1000] loss: 0.883\n","Accuracy of the network: 61 %\n","[6,  2000] loss: 0.840\n","Accuracy of the network: 64 %\n","[7,  1000] loss: 0.796\n","Accuracy of the network: 62 %\n","[7,  2000] loss: 0.786\n","Accuracy of the network: 63 %\n","[8,  1000] loss: 0.794\n","Accuracy of the network: 64 %\n","[8,  2000] loss: 0.806\n","Accuracy of the network: 66 %\n","[9,  1000] loss: 0.733\n","Accuracy of the network: 65 %\n","[9,  2000] loss: 0.718\n","Accuracy of the network: 66 %\n","[10,  1000] loss: 0.698\n","Accuracy of the network: 67 %\n","[10,  2000] loss: 0.698\n","Accuracy of the network: 67 %\n","[11,  1000] loss: 0.693\n","Accuracy of the network: 66 %\n","[11,  2000] loss: 0.696\n","Accuracy of the network: 69 %\n","[12,  1000] loss: 0.677\n","Accuracy of the network: 67 %\n","[12,  2000] loss: 0.665\n","Accuracy of the network: 69 %\n","[13,  1000] loss: 0.656\n","Accuracy of the network: 68 %\n","[13,  2000] loss: 0.656\n","Accuracy of the network: 70 %\n","[14,  1000] loss: 0.638\n","Accuracy of the network: 68 %\n","[14,  2000] loss: 0.652\n","Accuracy of the network: 69 %\n","[15,  1000] loss: 0.630\n","Accuracy of the network: 70 %\n","[15,  2000] loss: 0.625\n","Accuracy of the network: 69 %\n","[16,  1000] loss: 0.623\n","Accuracy of the network: 69 %\n","[16,  2000] loss: 0.618\n","Accuracy of the network: 69 %\n","[17,  1000] loss: 0.621\n","Accuracy of the network: 71 %\n","[17,  2000] loss: 0.606\n","Accuracy of the network: 69 %\n","[18,  1000] loss: 0.608\n","Accuracy of the network: 71 %\n","[18,  2000] loss: 0.601\n","Accuracy of the network: 70 %\n","[19,  1000] loss: 0.600\n","Accuracy of the network: 69 %\n","[19,  2000] loss: 0.601\n","Accuracy of the network: 70 %\n","[20,  1000] loss: 0.593\n","Accuracy of the network: 69 %\n","[20,  2000] loss: 0.591\n","Accuracy of the network: 69 %\n","[21,  1000] loss: 0.580\n","Accuracy of the network: 70 %\n","[21,  2000] loss: 0.596\n","Accuracy of the network: 70 %\n","[22,  1000] loss: 0.578\n","Accuracy of the network: 70 %\n","[22,  2000] loss: 0.582\n","Accuracy of the network: 70 %\n","[23,  1000] loss: 0.570\n","Accuracy of the network: 71 %\n","[23,  2000] loss: 0.576\n","Accuracy of the network: 70 %\n","[24,  1000] loss: 0.568\n","Accuracy of the network: 70 %\n","[24,  2000] loss: 0.568\n","Accuracy of the network: 70 %\n","[25,  1000] loss: 0.563\n","Accuracy of the network: 70 %\n","[25,  2000] loss: 0.567\n","Accuracy of the network: 69 %\n","[26,  1000] loss: 0.559\n","Accuracy of the network: 70 %\n","[26,  2000] loss: 0.555\n","Accuracy of the network: 71 %\n","[27,  1000] loss: 0.550\n","Accuracy of the network: 70 %\n","[27,  2000] loss: 0.551\n","Accuracy of the network: 71 %\n","[28,  1000] loss: 0.545\n","Accuracy of the network: 69 %\n","[28,  2000] loss: 0.553\n","Accuracy of the network: 69 %\n","[29,  1000] loss: 0.539\n","Accuracy of the network: 70 %\n","[29,  2000] loss: 0.542\n","Accuracy of the network: 69 %\n","[30,  1000] loss: 0.537\n","Accuracy of the network: 70 %\n","[30,  2000] loss: 0.540\n","Accuracy of the network: 70 %\n","[31,  1000] loss: 0.532\n","Accuracy of the network: 70 %\n","[31,  2000] loss: 0.539\n","Accuracy of the network: 70 %\n","[32,  1000] loss: 0.523\n","Accuracy of the network: 70 %\n","[32,  2000] loss: 0.531\n","Accuracy of the network: 69 %\n","[33,  1000] loss: 0.522\n","Accuracy of the network: 69 %\n","[33,  2000] loss: 0.523\n","Accuracy of the network: 70 %\n","[34,  1000] loss: 0.513\n","Accuracy of the network: 69 %\n","[34,  2000] loss: 0.524\n","Accuracy of the network: 69 %\n","[35,  1000] loss: 0.511\n","Accuracy of the network: 70 %\n","[35,  2000] loss: 0.514\n","Accuracy of the network: 69 %\n","[36,  1000] loss: 0.509\n","Accuracy of the network: 70 %\n","[36,  2000] loss: 0.513\n","Accuracy of the network: 69 %\n","[37,  1000] loss: 0.507\n","Accuracy of the network: 70 %\n","[37,  2000] loss: 0.504\n","Accuracy of the network: 69 %\n","[38,  1000] loss: 0.506\n","Accuracy of the network: 69 %\n","[38,  2000] loss: 0.494\n","Accuracy of the network: 69 %\n","[39,  1000] loss: 0.494\n","Accuracy of the network: 69 %\n","[39,  2000] loss: 0.494\n","Accuracy of the network: 70 %\n","[40,  1000] loss: 0.484\n","Accuracy of the network: 70 %\n","[40,  2000] loss: 0.492\n","Accuracy of the network: 69 %\n","[41,  1000] loss: 0.482\n","Accuracy of the network: 70 %\n","[41,  2000] loss: 0.491\n","Accuracy of the network: 68 %\n","[42,  1000] loss: 0.478\n","Accuracy of the network: 69 %\n","[42,  2000] loss: 0.482\n","Accuracy of the network: 70 %\n","[43,  1000] loss: 0.475\n","Accuracy of the network: 70 %\n","[43,  2000] loss: 0.476\n","Accuracy of the network: 68 %\n","[44,  1000] loss: 0.465\n","Accuracy of the network: 69 %\n","[44,  2000] loss: 0.473\n","Accuracy of the network: 69 %\n","[45,  1000] loss: 0.460\n","Accuracy of the network: 68 %\n","[45,  2000] loss: 0.470\n","Accuracy of the network: 68 %\n","[46,  1000] loss: 0.454\n","Accuracy of the network: 69 %\n","[46,  2000] loss: 0.464\n","Accuracy of the network: 68 %\n","[47,  1000] loss: 0.456\n","Accuracy of the network: 68 %\n","[47,  2000] loss: 0.456\n","Accuracy of the network: 69 %\n","[48,  1000] loss: 0.452\n","Accuracy of the network: 69 %\n","[48,  2000] loss: 0.453\n","Accuracy of the network: 68 %\n","[49,  1000] loss: 0.446\n","Accuracy of the network: 69 %\n","[49,  2000] loss: 0.447\n","Accuracy of the network: 68 %\n","[50,  1000] loss: 0.436\n","Accuracy of the network: 69 %\n","[50,  2000] loss: 0.441\n","Accuracy of the network: 69 %\n","[51,  1000] loss: 0.433\n","Accuracy of the network: 68 %\n","[51,  2000] loss: 0.434\n","Accuracy of the network: 68 %\n","[52,  1000] loss: 0.425\n","Accuracy of the network: 68 %\n","[52,  2000] loss: 0.432\n","Accuracy of the network: 69 %\n","[53,  1000] loss: 0.425\n","Accuracy of the network: 69 %\n","[53,  2000] loss: 0.422\n","Accuracy of the network: 68 %\n","[54,  1000] loss: 0.426\n","Accuracy of the network: 69 %\n","[54,  2000] loss: 0.424\n","Accuracy of the network: 68 %\n","[55,  1000] loss: 0.411\n","Accuracy of the network: 68 %\n","[55,  2000] loss: 0.415\n","Accuracy of the network: 69 %\n","[56,  1000] loss: 0.406\n","Accuracy of the network: 69 %\n","[56,  2000] loss: 0.418\n","Accuracy of the network: 68 %\n","[57,  1000] loss: 0.408\n","Accuracy of the network: 69 %\n","[57,  2000] loss: 0.405\n","Accuracy of the network: 68 %\n","[58,  1000] loss: 0.397\n","Accuracy of the network: 68 %\n","[58,  2000] loss: 0.403\n","Accuracy of the network: 68 %\n","[59,  1000] loss: 0.397\n","Accuracy of the network: 68 %\n","[59,  2000] loss: 0.395\n","Accuracy of the network: 69 %\n","[60,  1000] loss: 0.384\n","Accuracy of the network: 68 %\n","[60,  2000] loss: 0.394\n","Accuracy of the network: 69 %\n","[61,  1000] loss: 0.388\n","Accuracy of the network: 69 %\n","[61,  2000] loss: 0.388\n","Accuracy of the network: 68 %\n","[62,  1000] loss: 0.376\n","Accuracy of the network: 69 %\n","[62,  2000] loss: 0.388\n","Accuracy of the network: 68 %\n","[63,  1000] loss: 0.371\n","Accuracy of the network: 68 %\n","[63,  2000] loss: 0.385\n","Accuracy of the network: 68 %\n","[64,  1000] loss: 0.374\n","Accuracy of the network: 69 %\n","[64,  2000] loss: 0.374\n","Accuracy of the network: 69 %\n","[65,  1000] loss: 0.362\n","Accuracy of the network: 68 %\n","[65,  2000] loss: 0.370\n","Accuracy of the network: 69 %\n","[66,  1000] loss: 0.355\n","Accuracy of the network: 68 %\n","[66,  2000] loss: 0.365\n","Accuracy of the network: 68 %\n","[67,  1000] loss: 0.355\n","Accuracy of the network: 68 %\n","[67,  2000] loss: 0.356\n","Accuracy of the network: 68 %\n","[68,  1000] loss: 0.356\n","Accuracy of the network: 68 %\n","[68,  2000] loss: 0.352\n","Accuracy of the network: 68 %\n","[69,  1000] loss: 0.350\n","Accuracy of the network: 68 %\n","[69,  2000] loss: 0.351\n","Accuracy of the network: 68 %\n","[70,  1000] loss: 0.343\n","Accuracy of the network: 68 %\n","[70,  2000] loss: 0.348\n","Accuracy of the network: 68 %\n","[71,  1000] loss: 0.343\n","Accuracy of the network: 68 %\n","[71,  2000] loss: 0.344\n","Accuracy of the network: 68 %\n","[72,  1000] loss: 0.333\n","Accuracy of the network: 68 %\n","[72,  2000] loss: 0.343\n","Accuracy of the network: 68 %\n","[73,  1000] loss: 0.328\n","Accuracy of the network: 67 %\n","[73,  2000] loss: 0.335\n","Accuracy of the network: 67 %\n","[74,  1000] loss: 0.326\n","Accuracy of the network: 68 %\n","[74,  2000] loss: 0.330\n","Accuracy of the network: 68 %\n","[75,  1000] loss: 0.322\n","Accuracy of the network: 68 %\n","[75,  2000] loss: 0.327\n","Accuracy of the network: 68 %\n","[76,  1000] loss: 0.318\n","Accuracy of the network: 67 %\n","[76,  2000] loss: 0.325\n","Accuracy of the network: 67 %\n","[77,  1000] loss: 0.316\n","Accuracy of the network: 68 %\n","[77,  2000] loss: 0.314\n","Accuracy of the network: 67 %\n","[78,  1000] loss: 0.307\n","Accuracy of the network: 67 %\n","[78,  2000] loss: 0.317\n","Accuracy of the network: 67 %\n","[79,  1000] loss: 0.303\n","Accuracy of the network: 67 %\n","[79,  2000] loss: 0.306\n","Accuracy of the network: 67 %\n","[80,  1000] loss: 0.297\n","Accuracy of the network: 67 %\n","[80,  2000] loss: 0.307\n","Accuracy of the network: 67 %\n","[81,  1000] loss: 0.304\n","Accuracy of the network: 67 %\n","[81,  2000] loss: 0.301\n","Accuracy of the network: 68 %\n","[82,  1000] loss: 0.296\n","Accuracy of the network: 67 %\n","[82,  2000] loss: 0.292\n","Accuracy of the network: 68 %\n","[83,  1000] loss: 0.288\n","Accuracy of the network: 67 %\n","[83,  2000] loss: 0.295\n","Accuracy of the network: 68 %\n","[84,  1000] loss: 0.283\n","Accuracy of the network: 67 %\n","[84,  2000] loss: 0.291\n","Accuracy of the network: 67 %\n","[85,  1000] loss: 0.281\n","Accuracy of the network: 68 %\n","[85,  2000] loss: 0.285\n","Accuracy of the network: 67 %\n","[86,  1000] loss: 0.276\n","Accuracy of the network: 68 %\n","[86,  2000] loss: 0.278\n","Accuracy of the network: 67 %\n","[87,  1000] loss: 0.276\n","Accuracy of the network: 67 %\n","[87,  2000] loss: 0.278\n","Accuracy of the network: 67 %\n","[88,  1000] loss: 0.269\n","Accuracy of the network: 67 %\n","[88,  2000] loss: 0.279\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"gXkb3rJunJMY","colab_type":"code","colab":{}},"source":["def evaluate_model(data_loader, model):\n","    model.eval()\n","    correct = 0\n","    total = 0\n","    with torch.no_grad():\n","        for data in data_loader:\n","            train_input, labels = data\n","            hidden = model.init_hidden(labels.size(0)) # 有问题\n","            train_input, labels = train_input.to(device), labels.to(device)\n","            outputs = model(train_input, hidden)\n","            #print(outputs)\n","            _, predicted = torch.max(outputs.data, 1)\n","            total += labels.size(0)\n","            #print(predicted)\n","            #print(labels)\n","            correct += (predicted == labels).sum().item()\n","\n","    print('Accuracy of the network: %d %%' % (\n","    100 * correct / total))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"1Zk42pwE4CoS","colab_type":"code","outputId":"41193af0-26d5-4634-fbfa-b6c1bf41c270","executionInfo":{"status":"ok","timestamp":1583673429589,"user_tz":-480,"elapsed":1645,"user":{"displayName":"Rong Leslie","photoUrl":"","userId":"04972105603495756887"}},"colab":{"base_uri":"https://localhost:8080/","height":629}},"source":["evaluate_model(valid_loader, model)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["tensor([[0.0126, 0.9907, 0.1492],\n","        [0.0222, 0.9832, 0.1690],\n","        [0.0104, 0.9882, 0.1813],\n","        [0.0204, 0.9837, 0.2170],\n","        [0.0223, 0.9821, 0.2191],\n","        [0.0115, 0.9894, 0.1580],\n","        [0.0345, 0.9769, 0.1816],\n","        [0.0101, 0.9908, 0.1735],\n","        [0.0426, 0.9707, 0.2295],\n","        [0.0254, 0.9756, 0.2186],\n","        [0.0269, 0.9760, 0.2469],\n","        [0.0114, 0.9908, 0.1560],\n","        [0.0180, 0.9867, 0.1876],\n","        [0.0283, 0.9797, 0.2110],\n","        [0.0168, 0.9852, 0.1662],\n","        [0.0106, 0.9917, 0.1666],\n","        [0.0426, 0.9690, 0.2498],\n","        [0.0395, 0.9727, 0.2406],\n","        [0.0284, 0.9831, 0.2018],\n","        [0.0138, 0.9869, 0.1707],\n","        [0.0198, 0.9801, 0.1919],\n","        [0.0158, 0.9876, 0.2261],\n","        [0.0148, 0.9835, 0.1564],\n","        [0.0108, 0.9900, 0.1616],\n","        [0.0341, 0.9766, 0.2088],\n","        [0.0279, 0.9751, 0.2225],\n","        [0.0131, 0.9880, 0.2014],\n","        [0.0208, 0.9846, 0.2076],\n","        [0.0128, 0.9906, 0.1734],\n","        [0.0201, 0.9803, 0.2040],\n","        [0.0217, 0.9833, 0.1934],\n","        [0.0178, 0.9863, 0.2146]], device='cuda:0')\n","tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","        1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0')\n","tensor([1, 2, 1, 2, 0, 1, 1, 2, 1, 2, 1, 1, 2, 1, 1, 1, 0, 1, 1, 1, 1, 2, 0, 1,\n","        1, 1, 1, 2, 2, 1, 0, 1], device='cuda:0')\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"kxfkvwkp5mBf","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"dXn7jv6GnMMa","colab_type":"code","colab":{}},"source":["def predict_model(model, data_loader):\n","    model.eval()\n","    pred = []\n","    ids = []\n","    batch_num = len(test_data) // batch_size\n","    if(len(test_data) % batch_size != 0):\n","      batch_num += 1\n","    with torch.no_grad():\n","        for i in range(batch_num):\n","            if i == (batch_num - 1):\n","                batchs = test_data[i * batch_size: ]\n","            else:\n","                batchs = test_data[i * batch_size: (i + 1)* batch_size]\n","            current_size = len(batchs)\n","            hidden = model.init_hidden(current_size) # 有问题\n","            text_ids = [_[0] for _ in batchs]\n","            text = torch.LongTensor([_[1] for _ in batchs]).to(device)\n","            outputs = model(text, hidden)\n","            _, predicted = torch.max(outputs.data, 1)\n","            predict_val = list(predicted.cpu().numpy() - 1)\n","            ids = ids + text_ids\n","            pred = pred + predict_val\n","    result = pd.DataFrame({\n","        'id': ids,\n","        'y':pred\n","    })\n","    result.to_csv('reuslt.csv', index=False)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"aPEBpzQSnNMZ","colab_type":"code","colab":{}},"source":["test_data = TensorDataset(torch.from_numpy(test_content))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"dOvuZlCthdWE","colab_type":"code","colab":{}},"source":["test_data = DataLoader(test_data, shuffle=False, batch_size=batch_size)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"uV1RrNn6hq9U","colab_type":"code","colab":{}},"source":["predict_model(model,test_data)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ljmfxQWYiE38","colab_type":"code","outputId":"2f1402c6-1afb-41d8-85dc-b1f7ab6f1e18","executionInfo":{"status":"ok","timestamp":1583721185924,"user_tz":-480,"elapsed":1092,"user":{"displayName":"Rong Leslie","photoUrl":"","userId":"04972105603495756887"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["len(test_data)"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["10000"]},"metadata":{"tags":[]},"execution_count":29}]},{"cell_type":"code","metadata":{"id":"DNXNgPmvik8q","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}